from hanziconv import HanziConv
from multiprocessing import Pool
from collections import Counter
import sys, re
import argparse
import json

from google_bert import BasicTokenizer

BUFSIZE = 40960000

def parse_config():
    parser = argparse.ArgumentParser()
    parser.add_argument('--src_file', type=str)
    parser.add_argument('--tgt_file', type=str)
    parser.add_argument('--nprocessors', type=int)
    return parser.parse_args()


_split_set = set(['！', '？', '。'])
def _is_split_point(ch):
    if ch in _split_set:
        return True
    return False

def work_news_char(line):
    "This function only works for news at char level"
    tokenizer = BasicTokenizer() 
    line = line.strip()
    if line == "":
        return [[]]
    try:
        line = json.loads(line)
        if "text" in line:
            line = line["text"].strip()
            if not line:
                return [[]]
        else:
            return [[]]
    except:
        return [[]]
    line = HanziConv.toSimplified(line)
    char_seq = tokenizer.tokenize(line)
    res = []
    sent = []
    for ch in char_seq:
        sent.append(ch)
        if len(sent)>=20 and _is_split_point(ch):
            res.append(sent)
            sent = []
    if sent:
        if len(sent) <= 3 and len(res)>0:
            res[-1].extend(sent)
        else:
            res.append(sent)
    res.append([]) # end of doc
    return res

def work_wiki_char(line):
    "This function only works for zhwiki at char level"
    tokenizer = BasicTokenizer() 
    line = line.strip()
    if line == "":
        return []
    if line.startswith("</doc>"):
        return [[]]
    if line.startswith("<doc id="):
        return [[]]
    line = HanziConv.toSimplified(line) 
    char_seq = tokenizer.tokenize(line)
    res = []
    sent = []
    for ch in char_seq:
        sent.append(ch)
        if len(sent)>=20 and _is_split_point(ch):
            res.append(sent)
            sent = []
    if sent:
        if len(sent) <= 3 and len(res)>0:
            res[-1].extend(sent)
        else:
            res.append(sent)
    
    return res

if __name__ == "__main__":
    args = parse_config()
    pool = Pool(args.nprocessors)
    stream = open(args.src_file, encoding='utf8')
    cnt = Counter()
    with open(args.tgt_file, 'w', encoding ='utf8') as fo:
        while True:
            lines = stream.readlines(BUFSIZE)
            if not lines:
                break
            res = pool.map(work_news_char, lines, len(lines)//args.nprocessors)
            if not res:
                continue
            all_lines = []
            for lines in res:
                all_lines.extend(lines)
            empty = True
            print(len(all_lines))
            for line in all_lines:
                if line:
                    cnt.update(line)
                    fo.write(' '.join(line)+'\n')
                    empty = False
                else: 
                    if not empty:
                        fo.write('\n')
                    empty = True
    with open(args.tgt_file+'_vocab', 'w', encoding ='utf8') as fo:
        for x, y in cnt.most_common():
            fo.write(x+'\t'+str(y)+'\n')
